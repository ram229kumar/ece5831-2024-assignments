{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_hat, y):\n",
    "    return np.sum((y_hat - y)**2)/y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.524999999999995"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([9,8,7,6])\n",
    "y_hat1 = np.array([1.2, 1.9, 2.9,  4.2]) \n",
    "mean_squared_error(y_hat1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.525"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat2 = np.array([2.2, 0.9, 2.9,  5.2]) \n",
    "mean_squared_error(y_hat2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y_hat, y):\n",
    "    return -np.sum(y*np.log(y_hat + 1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat1 = np.array([0.1, 0.7, 0.1, 0.1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3566748010815999"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(y_hat1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial derivatives when x0 = 3, x1 = 4\n",
    "\n",
    "def func_tmp1(x0):\n",
    "    return x0**2 + 4**2\n",
    "\n",
    "def func_tmp2(x1):\n",
    "    return 3**2 + x1**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x + h) - f(x - h))/(2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) \n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) \n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_numerical_diff(func_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_numerical_diff(func_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_numerical_gradient(func2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.1, step_num = 100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = _numerical_gradient(f, x)\n",
    "        x -= lr*grad  # x = x - lr*grad\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.65680105e-06, 2.02028609e-06])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([2800.0, 1000.0])\n",
    "# func2 = x0**2 + x1**2\n",
    "gradient_descent(func2, init_x, step_num=10000, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.w = np.random.randn(2, 3)\n",
    "\n",
    "\n",
    "    # for multi-dimensional x\n",
    "    def softmax(self, x):\n",
    "        if x.ndim == 2:\n",
    "            x = x.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            return y.T \n",
    "\n",
    "        x = x - np.max(x)  \n",
    "        return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "    def cross_entroy_error(self, y, t):\n",
    "        delta = 1e-7\n",
    "        batch_size = 1 if y.ndim == 1 else y.shape[0]\n",
    "\n",
    "        return -np.sum(t*np.log(y + delta)) / batch_size\n",
    "\n",
    "\n",
    "    # for multi-dimensional x\n",
    "    def numerical_gradient(self, f, x):\n",
    "        h = 1e-4 # 0.0001\n",
    "        grad = np.zeros_like(x)\n",
    "        \n",
    "        it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            tmp_val = x[idx]\n",
    "            x[idx] = float(tmp_val) + h\n",
    "            fxh1 = f(x) # f(x+h)\n",
    "            \n",
    "            x[idx] = tmp_val - h \n",
    "            fxh2 = f(x) # f(x-h)\n",
    "            grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "            \n",
    "            x[idx] = tmp_val \n",
    "            it.iternext()   \n",
    "            \n",
    "        return grad\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.w)\n",
    "    \n",
    "\n",
    "    def loss(self, x, y):\n",
    "        z = self.predict(x)\n",
    "        y_hat = self.softmax(z)\n",
    "        loss = self.cross_entroy_error(y_hat, y)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.57043036 -0.38615786 -1.87673757]\n",
      " [-0.36988329  0.02179949 -0.67380581]]\n"
     ]
    }
   ],
   "source": [
    "net = SimpleNet()\n",
    "print(net.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.32902342 -0.2661686  -1.44173941]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.7, 0.19])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1384805143751462"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([0, 1, 0])\n",
    "net.loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0, 0, 1])\n",
    "net.loss(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TwoLayerNet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activations:\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    # for multi-dimensional x\n",
    "    def softmax(self, x):\n",
    "        if x.ndim == 2:\n",
    "            x = x.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            return y.T \n",
    "\n",
    "        x = x - np.max(x)  \n",
    "        return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Errors:\n",
    "    def cross_entroy_error(self, y, t):\n",
    "        delta = 1e-7\n",
    "        batch_size = 1 if y.ndim == 1 else y.shape[0]\n",
    "\n",
    "        return -np.sum(t*np.log(y + delta)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import activations\n",
    "import errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "\n",
    "        self.params['w1'] = weight_init_std*np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "\n",
    "        self.params['w2'] = weight_init_std*np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        self.activations = activations.Activations()\n",
    "        self.errors = errors.Errors()\n",
    "\n",
    "    def predict(self, x):\n",
    "        w1, w2 = self.params['w1'], self.params['w2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, w1) + b1\n",
    "        z1 = self.activations.sigmoid(a1)\n",
    "        a2 = np.dot(z1, w2) + b2\n",
    "        y = self.activations.softmax(a2)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        y_hat = self.predict(x)\n",
    "\n",
    "        return self.errors.cross_entropy_error(y_hat, y)\n",
    "    \n",
    "\n",
    "    def accuracy(self, x, y):\n",
    "        y_hat = self.predict(x)\n",
    "        p = np.argmax(y_hat, axis=1)\n",
    "        y_p = np.argmax(y, axis=1)\n",
    "\n",
    "        return np.sum(p == y_p)/float(x.shape[0])\n",
    "    \n",
    "\n",
    "    # for multi-dimensional x\n",
    "    def _numerical_gradient(self, f, x):\n",
    "        h = 1e-4 # 0.0001\n",
    "        grad = np.zeros_like(x)\n",
    "        \n",
    "        it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            tmp_val = x[idx]\n",
    "            x[idx] = float(tmp_val) + h\n",
    "            fxh1 = f(x) # f(x+h)\n",
    "            \n",
    "            x[idx] = tmp_val - h \n",
    "            fxh2 = f(x) # f(x-h)\n",
    "            grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "            \n",
    "            x[idx] = tmp_val \n",
    "            it.iternext()   \n",
    "            \n",
    "        return grad\n",
    "    \n",
    "\n",
    "    def numerical_gradient(self, x, y):\n",
    "        loss_w = lambda w: self.loss(x, y)\n",
    "\n",
    "        grads = {}\n",
    "        grads['w1'] = self._numerical_gradient(loss_w, self.params['w1'])\n",
    "        grads['b1'] = self._numerical_gradient(loss_w, self.params['b1'])\n",
    "        grads['w2'] = self._numerical_gradient(loss_w, self.params['w2'])\n",
    "        grads['b2'] = self._numerical_gradient(loss_w, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the two layer net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        self.activations = Activations()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activations.sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # \n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.w) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.w.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  \n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None \n",
    "        self.y_hat = None    \n",
    "        self.y = None    \n",
    "        self.activations = Activations()\n",
    "        self.errors = Errors()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.y = y\n",
    "        self.y_hat = self.activations.softmax(x)\n",
    "        self.loss = self.errors.cross_entropy_error(self.y_hat, self.y)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.y.shape[0]\n",
    "        #if self.y.size == self.y_hat.size: # one hot encoding\n",
    "        \n",
    "        dx = (self.y_hat - self.y) / batch_size\n",
    "        \n",
    "        \"\"\"\n",
    "        else:\n",
    "            dx = self.y_hat.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \"\"\"\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: train-images-idx3-ubyte.gz already exists.\n",
      "File: train-labels-idx1-ubyte.gz already exists.\n",
      "File: t10k-images-idx3-ubyte.gz already exists.\n",
      "File: t10k-labels-idx1-ubyte.gz already exists.\n",
      "Pickle: dataset/mnist_pkl already exists.\n",
      "Loading...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import mnist_data as mnist\n",
    "\n",
    "my_mnist = mnist.MnistData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total there are 50 number of images out of which there are 38 images predicted correctly and there are 12 images predicted wrongly.\n"
     ]
    }
   ],
   "source": [
    "# Define the base directory for your handwriting images\n",
    "base_dir = 'Custom MNIST Samples'\n",
    "\n",
    "correctCount = 0\n",
    "wrongCount = 0\n",
    "totalImages = 0\n",
    "\n",
    "# Loop through each digit and each image, running module5-3.py\n",
    "for digit in range(10):\n",
    "    digit_dir = os.path.join(base_dir, f'Digit {digit}')\n",
    "    \n",
    "    if os.path.isdir(digit_dir):\n",
    "        for filename in os.listdir(digit_dir):\n",
    "            if filename.endswith('.jpg'):\n",
    "                image_path = filename\n",
    "                # print(image_path)\n",
    "                totalImages+=1\n",
    "                result = subprocess.run([\"python\",\"module6.py\",image_path,str(filename)[0]],capture_output=True,text=True)\n",
    "                output = result.stdout.strip()\n",
    "                # print(output)\n",
    "                if \"Success\" in output:\n",
    "                    correctCount+=1\n",
    "                else:\n",
    "                    wrongCount+=1\n",
    "            # if totalImages==10: break\n",
    "        # if totalImages==10: break\n",
    "\n",
    "print(f'In total there are {totalImages} number of images out of which there are {correctCount} images predicted correctly and there are {wrongCount} images predicted wrongly.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teachable-python-ece5831-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
