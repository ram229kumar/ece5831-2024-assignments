{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2798af28bd0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[520]/255, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TwoLayerNet in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"sigmoid\", input_shape=(784, )),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"SGD\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the data for training\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.to_categorical(y_train, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape 28x28 to 784\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize values to 0 .. 1\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test =  keras.utils.to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.4565 - accuracy: 0.7088\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.7122 - accuracy: 0.8515\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5298 - accuracy: 0.8726\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4532 - accuracy: 0.8837\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4104 - accuracy: 0.8903\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3825 - accuracy: 0.8956\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3625 - accuracy: 0.8999\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3472 - accuracy: 0.9028\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3349 - accuracy: 0.9059\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3247 - accuracy: 0.9080\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3160 - accuracy: 0.9102\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3084 - accuracy: 0.9121\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3015 - accuracy: 0.9136\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2954 - accuracy: 0.9152\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2899 - accuracy: 0.9169\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2846 - accuracy: 0.9183\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2799 - accuracy: 0.9198\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2752 - accuracy: 0.9211\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2709 - accuracy: 0.9226\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2668 - accuracy: 0.9233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27989bf0610>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 6 9]\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(predictions, axis=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9]\n"
     ]
    }
   ],
   "source": [
    "labels = np.argmax(y_test[0:10], axis=1)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions == labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet:\n",
    "    def __init__(self, batch_size=32, epochs=20):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.model = None\n",
    "        self._create_lenet()\n",
    "        self._compile()\n",
    "    \n",
    "\n",
    "    def _create_lenet(self):\n",
    "        self.model = Sequential([\n",
    "            Conv2D(filters=6, kernel_size=(5,5), \n",
    "                   activation='sigmoid', input_shape=(28, 28, 1), \n",
    "                   padding='same'),\n",
    "            AveragePooling2D(pool_size=(2, 2), strides=2),\n",
    "            \n",
    "            Conv2D(filters=16, kernel_size=(5,5), \n",
    "                   activation='sigmoid', \n",
    "                   padding='same'),\n",
    "            AveragePooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "            Flatten(),\n",
    "\n",
    "            Dense(120, activation='sigmoid'),\n",
    "            Dense(84, activation='sigmoid'),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    def _compile(self):\n",
    "        if self.model is None:\n",
    "            print('Error: Create a model first..')\n",
    "        \n",
    "        self.model.compile(optimizer='Adam',\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "        \n",
    "\n",
    "    def _preprocess(self):\n",
    "        # load mnist data\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "        # normalize\n",
    "        x_train = x_train/255.0\n",
    "        x_test = x_test/255.0\n",
    "\n",
    "        # add channel dim\n",
    "        self.x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)  \n",
    "        self.x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)  \n",
    "\n",
    "        # one-hot encoding\n",
    "        self.y_train = to_categorical(y_train, 10)\n",
    "        self.y_test = to_categorical(y_test, 10)\n",
    "\n",
    "    def train(self):\n",
    "        self._preprocess()\n",
    "        self.model.fit(self.x_train, self.y_train, \n",
    "                  batch_size=self.batch_size, \n",
    "                  epochs=self.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet = LeNet(batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 1.0604 - accuracy: 0.6446\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.2350 - accuracy: 0.9299\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.1591 - accuracy: 0.9526\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.1216 - accuracy: 0.9624\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0973 - accuracy: 0.9702\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 12s 12ms/step - loss: 0.0835 - accuracy: 0.9732\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 12s 12ms/step - loss: 0.0700 - accuracy: 0.9782\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0610 - accuracy: 0.9808\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0554 - accuracy: 0.9827\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0494 - accuracy: 0.9846\n"
     ]
    }
   ],
   "source": [
    "lenet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002798BB72C00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 64ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(lenet.model.predict(x_test[0:10]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.argmax(lenet.y_test[0:10], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(predictions == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total there are 50 number of images out of which there are 42 images predicted correctly and there are 8 images predicted wrongly.\n"
     ]
    }
   ],
   "source": [
    "# Define the base directory for your handwriting images\n",
    "base_dir = 'Custom KERAS Samples'\n",
    "\n",
    "correctCount = 0\n",
    "wrongCount = 0\n",
    "totalImages = 0\n",
    "\n",
    "# Loop through each digit and each image, running module5-3.py\n",
    "for digit in range(10):\n",
    "    digit_dir = os.path.join(base_dir, f'Digit {digit}')\n",
    "    \n",
    "    if os.path.isdir(digit_dir):\n",
    "        for filename in os.listdir(digit_dir):\n",
    "            if filename.endswith('.jpg'):\n",
    "                image_path = filename\n",
    "                # print(image_path)\n",
    "                totalImages+=1\n",
    "                result = subprocess.run([\"python\",\"module8.py\",image_path,str(filename)[0]],capture_output=True,text=True)\n",
    "                output = result.stdout.strip()\n",
    "                # print(output)\n",
    "                if \"Success\" in output:\n",
    "                    correctCount+=1\n",
    "                else:\n",
    "                    wrongCount+=1\n",
    "            # if totalImages==10: break\n",
    "        # if totalImages==10: break\n",
    "\n",
    "print(f'In total there are {totalImages} number of images out of which there are {correctCount} images predicted correctly and there are {wrongCount} images predicted wrongly.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teachable-python-ece5831-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
